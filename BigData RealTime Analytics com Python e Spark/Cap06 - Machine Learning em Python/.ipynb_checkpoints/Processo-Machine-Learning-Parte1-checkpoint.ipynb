{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning em Python - Parte 1 - Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/processo.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url = 'images/processo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn as sl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sl.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do Problema de Negócio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um modelo preditivo que seja capaz de prever se uma pessoa pode ou não desenvolver diabetes. Para isso, usaremos dados históricos de pacientes, disponíveis no dataset abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Pima Indians Diabetes Data Set\n",
    "http://archive.ics.uci.edu/ml/datasets/diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataset descreve os registros médicos entre pacientes do Pima Inidians e cada registro está marcado se o paciente desenvolveu ou não diabetes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informações sobre os atributos:\n",
    "\n",
    "1. Number of times pregnant \n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. Diastolic blood pressure (mm Hg) \n",
    "4. Triceps skin fold thickness (mm) \n",
    "5. 2-Hour serum insulin (mu U/ml) \n",
    "6. Body mass index (weight in kg/(height in m)^2) \n",
    "7. Diabetes pedigree function \n",
    "8. Age (years) \n",
    "9. Class variable (0 or 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo e Carregando os Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem diversas considerações ao se carregar dados para o processo de Machine Learning. Por exemplo: seus dados possuem um header (cabeçalho)? Caso negativo, você vai precisar definir o título para cada coluna. Seus arquivos possuem comentários? Qual o delimitador das colunas? Alguns dados estão entre aspas, simples ou duplas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando arquivo csv usando NumPy\n",
    "import numpy as np\n",
    "arquivo = 'data/pima-data.csv'\n",
    "arquivo_data = open(arquivo, 'rb')\n",
    "dados = np.loadtxt(arquivo_data, delimiter = \",\")\n",
    "print(dados.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando arquivo csv usando Pandas\n",
    "import pandas as pd\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = pd.read_csv(arquivo, names = colunas)\n",
    "print(dados.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando arquivo csv usando Pandas (método que usaremos neste notebook)\n",
    "from pandas import read_csv\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "print(dados.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estatística Descritiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando as primeiras 20 linhas\n",
    "dados.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se o número de linhas no seu arquivo for muito grande, o algoritmo pode levar muito tempo para ser treinado. Se o número de registros for muito pequeno, você pode não ter registros suficientes para treinar seu modelo.\n",
    "\n",
    "Se você tiver muitas colunas em seu arquivo, o algoritmo pode apresentar problemas de performance devido a alta dimensionalidade.\n",
    "\n",
    "A melhor solução vai depender de cada caso. Mas lembre-se: treine seu modelo em um subset do seu conjunto de dados maior e depois aplique o modelo a novos dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando as dimensões\n",
    "dados.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tipo dos dados é muito importante. Pode ser necessário converter strings, ou colunas com números inteiros podem representar variáveis categóricas ou valores ordinários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipo de dados de cada atributo\n",
    "dados.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumário estatístico\n",
    "dados.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em problemas de classificação pode ser necessário balancear as classes. Classes desbalanceadas (ou seja, volume maior de um dos tipos das classes) são comuns e precisam ser tratadas durante a fase de pré-processamento. Podemos ver abaixo que existe uma clara desproporção entre as classes 0 (não ocorrência de diabetes) e 1 (ocorrência de diabetes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição das classes\n",
    "dados.groupby('class').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlação é o relacionamento entre 2 variáveis. O método mais comum para calcular correlação é o método de Pearson, que assume uma distribuição normal dos dados. Correlação de -1 mostra uma correlação negativa, enquanto uma correlação de +1 mostra uma correlação positiva. Uma correlação igual a 0 mostra que não há relacionamento entre as variáveis.\n",
    "\n",
    "Alguns algoritmos como regressão linear e regressão logística podem apresentar problemas de performance se houver atributos altamente correlacionados (colineares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlação de Pearson\n",
    "dados.corr(method = 'pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skew (ou simetria) se refere a distribuição dos dados que é assumida ser normal ou gaussiana (bell curve). Muitos algoritmos de Machine Learning consideram que os dados possuem uma distribuição normal. Conhecendo a simetria dos dados, permite que você faça uma preparação e entregue o que o algoritmo espera receber, aumentado desta forma a acurácia do modelo preditivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o skew de cada atributo\n",
    "dados.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização com Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Por se tratar de um conjunto de gráficos menores, pode ser mais interessante gerar os gráficos em janela separada\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o histograma podemos rapidamente avaliar a distribuição de cada atributo. Os histogramas agrupam os dados em bins e fornecem uma contagem do número de observações em cada bin. Com o histograma, você pode rapidamente verificar a simetria dos dados e se eles estão em distribuição normal ou não. Isso também vai ajudar na identificação dos outliers.\n",
    "\n",
    "Podemos ver que os atributos age, pedi e test possuem uma distribuição exponencial. Podemos ver que as colunas mass e press possuem uma distribuição normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma Univariado\n",
    "dados.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os Density Plots são outra forma de visualizar a distribuição dos dados para cada atributo. O plot é como uma espécie de histograma abstrato com uma curva suave através do topo dos bins de um histograma. Pode ser mais fácil identificar a distribuição dos dados usando um density plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density Plot Univariado\n",
    "dados.plot(kind = 'density', subplots = True, layout = (3,3), sharex = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os boxplots também podemos revisar a distribuição dos dados para cada atributo. A linha no centro (vermelho) é o valor da mediana (quartil 50%), a linha abaixo é o quartil 25% e a linha acima o quartil 75%. O boxplot ajuda a ter uma ideia da dispersão dos dados e os possíveis outliers.\n",
    "\n",
    "Podemos ver que a dispersão dos dados é bem diferente entre os atributos. As colunas age, skin e test possuem uma simetria muito próxima a valores de dados menores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and Whisker Plots\n",
    "dados.plot(kind = 'box', subplots = True, layout = (3,3), sharex = False, sharey = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de Correlação com nomes das variáveis\n",
    "correlations = dados.corr()\n",
    "\n",
    "# Plot\n",
    "import numpy as np\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin = -1, vmax = 1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0, 9, 1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(colunas)\n",
    "ax.set_yticklabels(colunas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de Correlação genérica\n",
    "correlations = dados.corr()\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin = -1, vmax = 1)\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um scatterplot mostra o relacionamento entre duas variáveis como pontos em duas dimensões, sendo um eixo para cada variável. Podemos criar um scatterplot para cada par de variáveis em nosso dataset. A exemplo da matriz de correlação, o scatterplot matrix é simétrico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot \n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(dados)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização com Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "sns.pairplot(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot com orientação vertical\n",
    "sns.boxplot(data = dados, orient = \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustermap\n",
    "sns.clustermap(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "sns.distplot(dados.pedi, fit = stats.norm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando os Dados para Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitos algoritmos esperam receber os dados em um formato específico. É seu trabalho preparar os dados em uma estrutura que seja adequada ao algoritmo que você está utilizando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É muito provável que você tenha que realizar tarefas de pré-processamento nos dados. Esse é um passo necessário dentro do processo. O desafio é o fato que cada algoritmo requer uma estrutura diferente, o que pode requerer transformações diferentes nos dados. Mas é possível em alguns casos, obter bons resultados sem um trabalho de pré-processamento. Mas é uma boa prática criar diferentes visões e transformações dos dados, de modo a poder testar diferentes algoritmos de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização - Método 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E uma das primeiras tarefas dentro do pré-processamento, é colocar seus dados na mesma escala. Muitos algoritmos de Machine Learning vão se beneficiar disso e produzir resultados melhores. Esta etapa também é chamada de normalização e significa colocar os dados em uma escala com range entre 0 e 1. Isso é útil para a otimização, sendo usado no core dos algoritmos de Machine Learning, como gradient descent. Isso também é útil para algoritmos como regressão e redes neurais e algoritmos que usam medidas de distância, como KNN. O scikit-learn possui uma função para esta etapa, chamada MinMaxScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando os dados para a mesma escala (entre 0 e 1)\n",
    "\n",
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input (X) e output (Y)\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Gerando a nova escala (normalizando os dados)\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    "# Sumarizando os dados transformados\n",
    "print(\"Dados Originais: \\n\\n\", dados.values)\n",
    "print(\"\\nDados Normalizados: \\n\\n\", rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização - Método 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No scikit-learn, normalização se refere a ajustar a escala de cada observação (linha) de modo que ela tenha comprimento igual a 1 (chamado vetor de comprimento 1 em álgebra linear). Este método de pré-processamento é útil quando temos datasets esparsos (com muitos zeros) e atributos com escala muito variada. Útil quando usamos algoritmos de redes neurais ou que usam medida de distância, como KNN. O scikit-learn possui uma função para esta etapa, chamada Normalizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando os dados (comprimento igual a 1)\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Gerando os dados normalizados\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)\n",
    "\n",
    "# Sumarizando os dados transformados\n",
    "print(\"Dados Originais: \\n\\n\", dados.values)\n",
    "print(\"\\nDados Normalizados: \\n\\n\", normalizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padronização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padronização é a técnica para transformar os atributos com distribuição Gaussiana (normal) e diferentes médias e desvios padrão em uma distribuição Gaussiana com a média igual a 0 e desvio padrão igual a 1. Isso é útil para algoritmos que esperam que os dados estejam com uma distribuição Gaussiana, como regressão linear, regressão logística e linear discriminant analysis. Funciona bem quando os dados já estão na mesma escala. O scikit-learn possui uma função para esta etapa, chamada StandardScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronizando os dados (0 para a média, 1 para o desvio padrão)\n",
    "\n",
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Gerando o novo padrão\n",
    "scaler = StandardScaler().fit(X)\n",
    "standardX = scaler.transform(X)\n",
    "\n",
    "# Sumarizando os dados transformados\n",
    "print(\"Dados Originais: \\n\\n\", dados.values)\n",
    "print(\"\\nDados Padronizados: \\n\\n\", standardX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarização (Transformar os Dados em Valores Binários)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós podemos definir um valor em nossos dados, ao qual chamamos de threshold e então definimos que todos os valores acima do threshold serão marcados como sendo 1 e todos valores iguais ou abaixo do threshold serão marcados como sendo 0. Isso é o que chamamos de Binarização. Isso é útil quando temos probabilidades e queremos transformar os dados em algo com mais significado. O scikit-learn possui uma função para esta etapa, chamada Binarizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarização\n",
    "\n",
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Gerando a binarização\n",
    "binarizer = Binarizer(threshold = 0.2).fit(X)\n",
    "binaryX = binarizer.transform(X)\n",
    "\n",
    "# Sumarizando os dados transformados\n",
    "print(\"Dados Originais: \\n\\n\", dados.values)\n",
    "print(\"\\nDados Binarizados: \\n\\n\", binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os atributos presentes no seu dataset e que você utiliza nos dados de treino, terão grande influência na precisão e resultado do seu modelo preditivo. Atributos irrelevantes terão impacto negativo na performance, enquanto atributos colineares podem afetar o grau de acurácia do modelo. O Scikit-learn possui funções que automatizam o trabalho de extração e seleção de variáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A etapa de Feature Selection é onde selecionamos os atributos (variáveis) que serão melhores candidatas a variáveis preditoras. O Feature Selection nos ajuda a reduzir o overfitting (quando o algoritmo aprende demais), aumenta a acurácia do modelo e reduz o tempo de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção Univariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testes estatísticos podem ser usados para selecionar os atributos que possuem forte relacionamento com a variável que estamos tentando prever. O Scikit-learn fornece a função SelectKBest() que pode ser usada com diversos testes estatísticos, para selecionar os atributos. Vamos usar o teste qui-quadrado e selecionar os 4 melhores atributos que podem ser usados como variáveis preditoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração de Variáveis com Testes Estatísticos Univariados (Teste qui-quadrado neste exemplo)\n",
    "\n",
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Função para seleção de variáveis\n",
    "best_var = SelectKBest(score_func = chi2, k = 4)\n",
    "\n",
    "# Executa a função de pontuação em (X, y) e obtém os recursos selecionados\n",
    "fit = best_var.fit(X, Y)\n",
    "\n",
    "# Reduz X para os recursos selecionados\n",
    "features = fit.transform(X)\n",
    "\n",
    "# Resultados\n",
    "print('\\nNúmero original de features:', X.shape[1])\n",
    "print('\\nNúmero reduzido de features:', features.shape[1])\n",
    "print('\\nFeatures (Variáveis Selecionadas): \\n\\n', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminação Recursiva de Atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta é outra técnica para seleção de atributos, que recursivamente remove os atributos e constrói o modelo com os atributos remanescentes. Esta técnica utiliza a acurácia do modelo para identificar os atributos que mais contribuem para prever a variável alvo. Em inglês esta técnia é chamada Recursive Feature Elimination (RFE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O exemplo abaixo utiliza a técnica de eliminação recursiva de atributos com um algoritmo de Regressão Logística para selecionar as 3 melhores variáveis preditoras. O RFE selecionou as variáveis preg, mass e pedi, que estão marcadas como True em \"Atributos Selecionados\" e com valor 1 em \"Ranking dos Atributos\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminação Recursiva de Variáveis\n",
    "\n",
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Criação do modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# RFE\n",
    "rfe = RFE(modelo, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "\n",
    "# Print dos resultados\n",
    "print(\"Variáveis Preditoras:\", dados.columns[0:8])\n",
    "print(\"Variáveis Selecionadas: %s\" % fit.support_)\n",
    "print(\"Ranking dos Atributos: %s\" % fit.ranking_)\n",
    "print(\"Número de Melhores Atributos: %d\" % fit.n_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método Ensemble para Seleção de Variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagged Decision Trees, como o algoritmo RandomForest (esses são chamados de Métodos Ensemble), podem ser usados para estimar a importância de cada atributo. Esse método retorna um score para cada atributo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quanto maior o score, maior a importância do atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importância do Atributo com o Extra Trees Classifier\n",
    "\n",
    "# Import dos Módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Criação do Modelo - Feature Selection\n",
    "modelo = ExtraTreesClassifier()\n",
    "modelo.fit(X, Y)\n",
    "\n",
    "# Print dos Resultados\n",
    "print(dados.columns[0:8])\n",
    "print(modelo.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redução de Dimensionalidade (Feature Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O PCA foi inventado em 1901 por Karl Pearson e utiliza álgebra linear para transformar datasets em uma forma comprimida, o que é geralmente conhecido como Redução de Dimensionalidade. Com PCA você pode escolher o número de dimensões (chamados componentes principais) no resultado transformado. Vamos usar PCA para selecionar 3 componentes principais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Análise de Componentes Principais (PCA) é um método para extração das variáveis importantes (na forma de componentes) a partir de um grande conjunto de variáveis, disponíveis em um conjunto de dados. Esta técnica permite extrair um número pequenos de conjuntos dimensionais a partir de um dataset altamente dimensional. Com menos variáveis a visualização também se torna muito mais significativa. PCA é mais útil quando se lida com 3 ou mais dimensões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image source: http://www.nlpca.org/pca_principal_component_analysis.html\n",
    "from IPython.display import Image\n",
    "Image(url = 'images/PCA2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada componente resultante é uma combinação linear de n atributos. Ou seja, cada componente principal é uma combinação de atributos presentes no dataset. O Primeiro Componente Principal é a combinação linear dos atributos com máxima variância e determina a direção em que há mais alta variabilidade nos dados. Quanto maior a variabilidade capturada no primeiro componente principal, mais informação será capturada pelo componente. O Segundo Componente Principal captura a variabilidade remanescente. Todos os componentes subsequentes possuem o mesmo conceito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image source: http://www.nlpca.org/pca_principal_component_analysis.html\n",
    "from IPython.display import Image\n",
    "Image(url = 'images/PCA3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O PCA precisa ser alimentado com dados normalizados. Utilizar o PCA em dados não normalizados pode gerar resultados inesperados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise de componentes principais é uma técnica da estatística multivariada que consiste em transformar um conjunto de variáveis originais em outro conjunto de variáveis denominadas de componentes principais. Os componentes principais apresentam propriedades importantes: cada componente principal é uma combinação linear de todas as variáveis originais, são independentes entre si e estimados com o propósito de reter, em ordem de estimação, o máximo de informação, em termos da variação total contida nos dados. Os componentes principais são garantidamente independentes apenas se os dados forem normalmente distribuídos (conjuntamente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procura-se redistribuir a variação observada nos eixos originais de forma a se obter um conjunto de eixos ortogonais não correlacionados. Esta técnica pode ser utilizada para geração de índices e agrupamento de indivíduos. A análise agrupa os indivíduos de acordo com sua variação, isto é, os indivíduos são agrupados segundo suas variâncias, ou seja, segundo seu comportamento dentro da população, representado pela variação do conjunto de características que define o indivíduo, ou seja, a técnica agrupa os indivíduos de uma população segundo a variação de suas características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise de componentes principais é associada à idéia de redução de massa de dados, com menor perda possível da informação. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo é sumarizar os dados que contém muitas variáveis (p) por um conjunto menor de variáveis (k) compostas derivadas a partir do conjunto original. PCA usa uma conjunto de dados representado por uma matriz de n registros por p atributos, que podem estar correlacionados, e sumariza esse conjunto por eixos não correlacionados (componentes principais) que são uma combinação linear das p variáveis originais. As primeiras k componentes contém a maior quantidade de variação dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em termos gerais a PCA busca reduzir o número de dimensões de um dataset, projetando os dados em um novo plano. Usando essa nova projeção os dados originais, que podem envolver diversas variáveis, podem ser interpretados utilizando menos \"dimensões.\"\n",
    "\n",
    "No dataset reduzido podemos observar com mais clareza tendências, padrões e/ou outliers. Mas vale lembrar que a regra: \"Se não está nos dados brutos não existe!\" é sempre válida. A PCA fornece apenas mais clareza aos padrões que já estão lá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url = 'images/PCA.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quanto maior a variância, maior a quantidade de informação contida no componente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Normalizando os dados\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    "# Seleção de atributos\n",
    "pca = PCA(n_components = 4)\n",
    "fit = pca.fit(rescaledX)\n",
    "\n",
    "# Sumarizando os componentes\n",
    "print(\"Variância: %s\" % fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amostragem - Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você precisa saber se seu modelo preditivo vai funcionar bem quando receber novos dados. A melhor maneira de avaliar a performance do modelo é fazer previsões em dados que você já conhece o resultado. Outra maneira de testar a performance do seu modelo é utilizar técnicas estatísticas como métodos de amostragem que permitem você estimar quão bem seu modelo irá fazer previsões em novos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A avaliação do modelo é uma estimativa de quão bem o algoritmo será capaz de prever em novos dados. Isso não garante performance. Após avaliar o modelo, nós podemos treiná-lo novamente com os dados de treino e então prepará-lo para uso operacional em produção. Existem diversas técnicas para isso e estudaremos duas aqui: Conjunto de dados de treino e de teste e Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados de Treino e de Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é o método mais utilizado para avaliar performance de um algoritmo de Machine Learning. Dividimos nossos dados originais em dados de treino e de teste. Treinamos o algoritmo nos dados de treino e fazemos as previsões nos dados de teste e avaliamos o resultado. A divisão dos dados vai depender do seu dataset, mas utiliza-se com frequência tamanhos entre 70/30 (treino/teste) e 65/35 (treino/teste)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método é bem veloz e ideal para conjuntos de dados muito grandes. O ponto negativo é a possibilidade de alta variância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação usando dados de treino e de teste\n",
    "\n",
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo o tamanho das amostras\n",
    "teste_size = 0.33\n",
    "\n",
    "# Garante que os resultados podem ser reproduzidos\n",
    "# Isso é importante para comparar a acurácia com outros algoritmos de Machine Learning.\n",
    "seed = 7\n",
    "\n",
    "# Criando os conjuntos de dados de treino e de teste\n",
    "X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size = teste_size, random_state = seed)\n",
    "\n",
    "# Criação do modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# Treinamento do modelo\n",
    "modelo.fit(X_treino, Y_treino)\n",
    "\n",
    "# Score do modelo nos dados de teste\n",
    "result = modelo.score(X_teste, Y_teste)\n",
    "print(\"Acurácia nos Dados de Teste: %.3f%%\" % (result * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation é uma técnica que pode ser utilizada para avaliar a performance de um modelo com menos variância que a técnica de dividir os dados em treino/teste. Com esta técnica dividimos os dados em partes normalmente chamadas de k-folds (por exemplo k = 5, k = 10). Cada parte é chamada fold. O algoritmo é treinado em k-1 folds. Cada fold é usado no treinamento de forma repetida e um fold por vez. Após executar o processo em k-1 folds, podemos sumarizar a performance em cada fold usando a média e o desvio padrão (Eu disse que Estatística era importante no processo de Big Data Analytics). O resultado é normalmente mais confiável e oferece maior acurácia ao modelo. A chave deste processo está em definir o correto valor de k, de modo que o número de folds represente adequadamente o número de repetições necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url = 'images/cross-validation.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação usando Cross Validation\n",
    "\n",
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para os folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
    "\n",
    "# Usamos a média e o desvio padrão\n",
    "print(\"Acurácia Final: %.3f%%\" % (resultado.mean() * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando a Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As métricas que você escolhe para avaliar a performance do modelo vão influenciar a forma como a performance é medida e comparada com modelos criados com outros algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o mesmo algoritmo, mas com métricas diferentes e assim comparar os resultados. A função cross_validation.cross_val_score() será usada para avaliar a performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas para Algoritmos de Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia\n",
    "# Número de previsões corretas. É útil apenas quando existe o mesmo número de observações em cada classe.\n",
    "\n",
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# Cross Validation\n",
    "resultado = cross_val_score(modelo, X, Y, cv = kfold, scoring = 'accuracy')\n",
    "\n",
    "# Print dos resultados\n",
    "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC \n",
    "# A Curva ROC permite analisar a métrica AUC (Area Under the Curve).\n",
    "# Essa é uma métrica de performance para classificação binária, em que podemos definir as classes em positiavs e negativas.\n",
    "# Problemas de classificação binária são um trade-off sentre Sensitivity e Specifity.\n",
    "# Sensitivity é a taxa de verdadeiros positivos (TP). Ese é o número de instâncias positivas da primeira classe que foram previstas corretamente.\n",
    "# Specifity é a taxa de verdadeiros negativos (TN). Esse é o número de instâncias da segunda classe que foram previstas corretamente.\n",
    "# Valores acima de 0.5 indicam uma boa taxa de previsão.\n",
    "\n",
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Cross Validation\n",
    "resultado = cross_val_score(model, X, Y, cv = kfold, scoring = 'roc_auc')\n",
    "\n",
    "# Print do resultado\n",
    "print(\"AUC: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "# Permite verificar a acurácia em um formato de tabela\n",
    "\n",
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo o tamanho do conjunto de dados\n",
    "teste_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "# Dividindo os dados em treino e teste\n",
    "X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size = teste_size, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "model = LogisticRegression()\n",
    "model.fit(X_treino, Y_treino)\n",
    "\n",
    "# Fazendo as previsões e construindo a Confusion Matrix\n",
    "previsoes = model.predict(X_teste)\n",
    "matrix = confusion_matrix(Y_teste, previsoes)\n",
    "\n",
    "# Imprimindo a Confusion Matrix\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relatório de Classificação\n",
    "\n",
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo o tamanho do conjunto de dados\n",
    "teste_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "# Dividindo os dados em treino e teste\n",
    "X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size = teste_size, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "modelo.fit(X_treino, Y_treino)\n",
    "\n",
    "# Fazendo as previsões e construindo o relatório\n",
    "previsoes = model.predict(X_teste)\n",
    "report = classification_report(Y_teste, previsoes)\n",
    "\n",
    "# Imprimindo o relatório\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos de Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não temos como saber qual algoritmo vai funcionar melhor na construção do modelo, antes de testarmos o algoritmo com nosso dataset. O ideal é testar alguns algoritmos e então escolher o que fornece melhor nível de precisão. Vamos testar um conjunto de algoritmos de classificação, nas mesmas condições."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo Linear. O algoritmo de Regressão Logística assume que seus dados estão em uma Distribuição Normal para valores numéricos que podem ser modelados com classificação binária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# Cross Validation\n",
    "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo Linear. Técnica estatística para classificação binária. Também assume que os dados estão em Distribuição Normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Cross Validation\n",
    "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN - K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo Não-Linear que utiliza uma métrica de distância para encontrar o valor de K mais adequado as instâncias do dataset de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "random_state = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = random_state)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = KNeighborsClassifier()\n",
    "\n",
    "# Cross Validation\n",
    "results = cross_val_score(modelo, X, Y, cv = kfold)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo Não-Linear. Calcula a Probabilidade de cada classe e a probabilidade condicional de cada classe dado uma variável de entrada. As probabilidades são então estimadas para os novos dados e multiplicadas, assumindo que são independentes (suposição simples ou Naive). Assume dados em distirbuição Gaussiana (Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = GaussianNB()\n",
    "\n",
    "# Cross Validation\n",
    "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART (Classification and Regression Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo Não-Linear. O algoritmo CART constrói uma árvore binária a partir do dataset de treino. Cada atributo e cada valor de cada atributo são avaliados com o objetivo de reduzir a função de custo (Cost Function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = DecisionTreeClassifier()\n",
    "\n",
    "# Cross Validation\n",
    "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo deste algoritmo é buscar uma linha que melhor separa duas classes dentro de um conjunto de dados. As instâncias de dados que estão mais próximas desta linha que separa as classes, são chamadas support vectors. O SVM tem sido estendido para suportar multiclasses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines são algoritmos de classificação muito poderosos. Quando usados em conjunto com “Random forest” e outras ferramentas de aprendizagem de máquina, dão uma dimensão muito diferente para montagem de modelos. Assim, eles se tornam cruciais para os casos em que é necessária um poder de previsão muito elevado. Esses algoritmos são um pouco mais difíceis de visualizar devido à complexidade na formulação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = SVC()\n",
    "\n",
    "# Cross Validation\n",
    "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleção do Modelo Preditivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veremos que os algoritmos de Regressão Logística e Linear Discriminant Analysis apresentaram o melhor nível de precisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Preparando a lista de modelos\n",
    "modelos = []\n",
    "modelos.append(('LR', LogisticRegression()))\n",
    "modelos.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "modelos.append(('NB', GaussianNB()))\n",
    "modelos.append(('KNN', KNeighborsClassifier()))\n",
    "modelos.append(('CART', DecisionTreeClassifier()))\n",
    "modelos.append(('SVM', SVC()))\n",
    "\n",
    "# Avaliando cada modelo em um loop\n",
    "resultados = []\n",
    "nomes = []\n",
    "\n",
    "for nome, modelo in modelos:\n",
    "    kfold = KFold(n_splits = num_folds, random_state = seed)\n",
    "    cv_results = cross_val_score(modelo, X, Y, cv = kfold, scoring = 'accuracy')\n",
    "    resultados.append(cv_results)\n",
    "    nomes.append(nome)\n",
    "    msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# Boxplot para comparar os algoritmos\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Comparação de Algoritmos de Classificação')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(resultados)\n",
    "ax.set_xticklabels(nomes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimização do Modelo - Ajuste de Hyperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos os algoritmos de Machine Learning são parametrizados, o que significa que você pode ajustar a performance do seu modelo preditivo, através do tuning (ajuste fino) dos parâmetros. Seu trabalho é encontrar a melhor combinação entre os parâmetros em cada algoritmo de Machine Learning. Esse processo também é chamado de Otimização de Hyperparâmetros. O scikit-learn oferece dois métodos para otimização automática dos parâmetros: Grid Search Parameter Tuning e Random Search Parameter Tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método realiza metodicamente combinações entre todos os parâmetros do algoritmo, criando um grid. Vamos experimentar este método utilizando o algoritmo de Regressão Logística. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores que serão testados\n",
    "valores_grid = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# Criando o grid\n",
    "grid = GridSearchCV(estimator = modelo, param_grid = valores_grid)\n",
    "grid.fit(X, Y)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (grid.best_score_ * 100))\n",
    "print(\"Melhores Parâmetros do Modelo:\\n\", grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método gera amostras dos parâmetros dos algoritmos a partir de uma distribuição randômica uniforme para um número fixo de iterações. Um modelo é construído e testado para cada combinação de parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores que serão testados\n",
    "seed = 7\n",
    "iterations = 14\n",
    "\n",
    "# Definindo os valores que serão testados\n",
    "valores_grid = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# Criando o grid\n",
    "rsearch = RandomizedSearchCV(estimator = modelo, \n",
    "                             param_distributions = valores_grid, \n",
    "                             n_iter = iterations, \n",
    "                             random_state = seed)\n",
    "rsearch.fit(X, Y)\n",
    "\n",
    "# Print dos resultados\n",
    "print(\"Acurácia: %.3f\" % (rsearch.best_score_ * 100))\n",
    "print(\"Melhores Parâmetros do Modelo:\\n\", rsearch.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando o resultado do seu trabalho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o resultado do seu trabalho\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo o tamanho dos dados de treino e de teste\n",
    "teste_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "# Criando o dataset de treino e de teste\n",
    "X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size = teste_size, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# Treinando o modelo\n",
    "modelo.fit(X_treino, Y_treino)\n",
    "\n",
    "# Salvando o modelo\n",
    "arquivo = 'modelos/modelo_classificador_final.sav'\n",
    "pickle.dump(modelo, open(arquivo, 'wb'))\n",
    "print(\"Modelo salvo!\")\n",
    "\n",
    "# Carregando o arquivo\n",
    "modelo_classificador_final = pickle.load(open(arquivo, 'rb'))\n",
    "modelo_prod = modelo_classificador_final.score(X_teste, Y_teste)\n",
    "print(\"Modelo carregado!\")\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (modelo_prod.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimizando Performance com Métodos Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métodos Ensemble permitem aumentar consideravelmente o nível de precisão nas suas previsões. Veremos como criar alguns dos Métodos Ensemble mais poderosos em Python. Existem 3 métodos principais para combinar previsões a partir de diferentes modelos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging - Para construção de múltiplos modelos (normalmente do mesmo tipo) a partir de diferentes subsets no dataset de treino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting - Para construção de múltiplos modelos (normalmente do mesmo tipo), onde cada modelo aprende a corrigir os erros gerados pelo modelo anterior, dentro da sequência de modelos criados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting - Para construção de múltiplos modelos (normalmente de tipos diferentes) e estatísticas simples (como a média) são usadas para combinar as previsões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vejamos como utilizar estes métodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagged Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método funciona bem quando existe alta variância nos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Cria o modelo unitário (classificador fraco)\n",
    "cart = DecisionTreeClassifier()\n",
    "\n",
    "# Definindo o número de trees\n",
    "num_trees = 100\n",
    "\n",
    "# Criando o modelo bagging\n",
    "modelo = BaggingClassifier(base_estimator = cart, n_estimators = num_trees, random_state = seed)\n",
    "\n",
    "# Cross Validation\n",
    "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest é uma extensão do Baggig Decision Tree. Amostras do dataset de treino são usadas com reposição, mas as árvores são criadas de uma forma que reduz a correlação entre classificadores individuais (Random Forest é um conjunto de árvores de decisão)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Definindo o número de trees\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = RandomForestClassifier(n_estimators = num_trees, max_features = max_features)\n",
    "\n",
    "# Cross Validation\n",
    "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmos baseados em Boosting Ensemble criam uma sequência de modelos que tentam corrigir os erros dos modelos anteriores dentro da sequência. Uma vez criados, os modelos fazem previsões que podem receber um peso de acordo com sua acurácia e os resultados são combinados para criar uma previsão única final. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O AdaBoost atribui pesos às instâncias no dataset, definindo quão fácil ou difícil elas são para o processo de classificação, permitindo que o algoritmo tenha mais ou menos atenção às instâncias durante o processo de construção dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Definindo o número de trees\n",
    "num_trees = 30\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = AdaBoostClassifier(n_estimators = num_trees, random_state = seed)\n",
    "\n",
    "# Cross Validation\n",
    "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também chamado Stochastic Gradient Boosting, é um dos métodos Ensemble mais sofisticados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Definindo o número de trees\n",
    "num_trees = 100\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = GradientBoostingClassifier(n_estimators = num_trees, random_state = seed)\n",
    "\n",
    "# Cross Validation\n",
    "resultado = cross_val_score(modelo, X, Y, cv = kfold)\n",
    "\n",
    "# Print do resultado\n",
    "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é um dos métodos Ensemble mais simples. Este método cria dois ou mais modelos separados a partir do dataset de treino. O Classificador Voting então utiliza a média das previsões de cada sub-modelo para fazer as previsões em novos conjuntos de dados. As previsões de cada sub-modelo podem receber pesos, através de parâmetros definidos manualmente ou através de heurística. Existem versões mais avançadas do Voting, em que o modelo pode aprender o melhor peso a ser atribuído aos sub-modelos. Isso é chamado Stacked Aggregation, mas ainda não está disponível no Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando os modelos\n",
    "estimators = []\n",
    "\n",
    "modelo1 = LogisticRegression()\n",
    "estimators.append(('logistic', modelo1))\n",
    "\n",
    "modelo2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', modelo2))\n",
    "\n",
    "modelo3 = SVC()\n",
    "estimators.append(('svm', modelo3))\n",
    "\n",
    "# Criando o modelo ensemble\n",
    "ensemble = VotingClassifier(estimators)\n",
    "\n",
    "# Cross Validation\n",
    "resultado = cross_val_score(ensemble, X, Y, cv = kfold)\n",
    "\n",
    "# Resultado\n",
    "print(\"Acurácia: %.3f\" % (resultado.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo XGBoost - Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo XGBoost é uma extensão do GBM (Gradient Boosting Method) que permite trabalhar com multithreading em uma única máquina e processamento paralelo em um cluster de vários servidores. A principal vantagem do XGBoost sobre o GBM é sua capacidade de gerenciar dados esparsos. O XGBoost automaticamente aceita dados esparsos como input sem armazenar zeros na memória."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principais vantagensdo XGBoost:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Aceita dados esparsos (o que permite trabalhar com matrizes esparsas), sem a necessidade de conversão para matrizes densas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Constrói uma árvore de aprendizagem utilizando um moderno método de split (chamado quatile sketch), o que resulta em tempo de processamento muito menor que métodos tradicionais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Permite computação paralela em uma única máquina (através do uso de multithreading) e processamento paralelo em máquinas distribuídas em cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basicamente o XGBoost utiliza os mesmos parâmetros do GBM e permite tratamento avançado de dados missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O XGBoost é muito utilizado por Cientistas de Dados que vencem competições no Kaggle. Repositório no Github: https://github.com/dmlc/XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar XGBoost a partir do PyPi\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = read_csv(arquivo, names = colunas)\n",
    "array = dados.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Definindo o tamanho dos dados de treino e de teste\n",
    "teste_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "# Criando o dataset de treino e de teste\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, Y, test_size = teste_size, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = XGBClassifier()\n",
    "\n",
    "# Treinando o modelo\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Pront do modelo\n",
    "print(modelo)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred = modelo.predict(X_teste)\n",
    "previsoes = [round(value) for value in y_pred]\n",
    "\n",
    "# Avaliando as previsões\n",
    "accuracy = accuracy_score(y_teste, previsoes)\n",
    "print(\"Acurácia: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obrigado - Data Science Academy - <a href=\"http://facebook.com/dsacademybr\">facebook.com/dsacademybr</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
